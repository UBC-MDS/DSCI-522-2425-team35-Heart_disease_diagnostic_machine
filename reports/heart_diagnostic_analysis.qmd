---
title: "Creating Machine learning Model to Predict Presence of Coronary Artery Disease"
author: "Marek Boulerice, Sarah Eshafi, Long Nguyen, Hui Tang"
format: html
editor: source
bibliography: references.bib
execute: 
  echo: false
---

## 1. SUMMARY

The following document covers a machine learning model analysis
with a goal to predict angiographic coronary disease in patients. 
Data is pulled from patients undergoing angiography at the Cleveland Clinic in Ohio. 
This analysis is composed of Exploratory Data Analysis, 
testing of various machine models on a training data set, 
model optimization via hyperparameter, 
and final model performance analysis. 
The final model is shown to have promising results, 
though limitations apply and further testing and optimization is recommended.

## 2. INTRODUCTION
Heart disease is the leading cause of death worldwide. 
Treating these heart diseases depends on capability of detecting symptoms and diagnosing cases earlier. 
One complication to diagnosing heart diseases is that many cases are found to be aymptomatic [@asymptomatic_coronary_disease]. 
This creates an opportunity for application of machine learning methods,
where the following question can be asked:
Given various details about a clients medical status, 
can we create a statistical model to accurately predict whether the patient has the disease? 
The goal of the following analysis is to create a model that can anwswer this question. 
To be best suited to the problem, the model should retain a high accuracy while minimizing the number of false negatives 
(ie predicting that a patient does not have the heart disease when the patient in fact does).

In particular, this analysis focuses on detection of angiographic coronary disease. 
The data set used in creating our model was taken from 303 patients undergoing angiography Cleveland Clinic in Cleveland, Ohio [@heart_disease_45].
From this procedure, a set of parameters were collected about each patients, 
and a diagnosis of whether the patient had the angiographic coronary disease (signified by a diameter narrowing of the coronary artery by at least 50%).
This is to serve as the target variable in our analysis

The set of parameters collected during the procedure,
used as our features for model training, are as follows:

- **Age (in years)** : Age of patient (years)
- **Sex** : Sex of patient (male or female)
- **Chest pain type**: categorical feature describing the type of pain experienced by the patient
- **Resting Blood Pressure**: numeric feature giving patients resting blood pressure
- **Serum Cholesterol** : numneric feature giving the patients Serum cholesterol in mg/dl
- **Fasting blood sugar > 120 mg/dl** : binary feature indicating whether the patients Blood sugar level while fasting exceeded 120 mg/dl
- **Resting electrocardiographic results**:  categorical feature reporting patients ECG results
- **Maximum heart rate achieved**: numeric feature giving maximum heart rate achieved by patent
- **Exercise-induced angina**: binary feature indicating whether patient underwent exercise induced angina
- **ST depression induced by exercise relative to rest**: numeric feature indicating the ST depression induced by exercise relative to rest
- **Slope of the peak exercise ST segment**
- **Thalassemia** : categorical feature indicating if patient suffered from Thalassemia


The following sections will discuss the decisions made and results in our Exploratory Data analysis, Machine learning model training, and final model performance

This report also drew information from the study done by [@OFlaherty2008] and [@thalassemia]


## 3. DATA VALIDATION & CLEANING

Prior to performing our analysis, an initial validation and cleaning step was performed on the data.

### INITIAL DATA CLEANING
From an initial preview of the data, some issues were correct immediately, before performing formal data validation. 
This includes removing invalid target values (i.e. values above 2) 
and converting the values to their semantic meaning.

Next, we evaluated whether the features in the train and test dataset are distributed similarly using deepchecks.
The check concluded that that the features were not distributed significantly and our distributions are as expected.


## 4. METHOD

The following section outlines the steps taken in manipulating our data and creating our model.

EDA is first conducted to obtain an idea of feature importance and to establish any important correlations to watch for.
Machine learning analysis is then performed, where multiple models are tested and their performance compared. 
The best performing model is selected to proceed with. 
On this model, hyperparameter test is completed via random search to tune our model and obtain best results.
Finally, the model is trained and tested on a separate data set, and evaluated for performance.

### 4.1 EDA
In this section, preliminary analysis is conducted 
to obtain an idea of possible correlations between features to be on the look out for. the results are presented below:

![]() -figure of eda


## 4.2 ML-Analysis

The following section outlines the procedure taken in creating our model and testing it on our data set.
As this is a classification problem (predict whether the patient has the disease or not), 
the chosen models for testing in this analyis are a Logistic Regression model and a Suport Vector Classifier. 
These two models were selected as they have been shown to historically perform well on real world data sets.

Since this data set is somewhat unbalanced (~80/20 split on non-disease vs disease), 
the primary scoring metric to evaluate these models will be F1 score, 
though model accuracy is still taken into consideration. 
Due to the nature of the analysis, special attention is taken to minimize false negatives as they represent the most damaging type of error
(predicting that a patient is free of the disease when he does in fact have it). 
To this effect, we look to maximize Recall.

The framework upon which this analysis is based has been adapted from DSCI573 Lab1.

### 4.2.2. Data Preprocessing
Features are sorted by type, 
and a column transformer object is created. 
on categorical columns, simple imputing is applied filling missing values with the most frequently occuring value. 
One hot encoding is then performed. 

For numerical feature, standard scaling is applied to keep all features within the same range

![]() - table of data head

### 4.2.3. model creation

Basic models (default hyperparameter values) are now generated. 
A dummy model is first created to use as a baseline to use for comparison.
Then, a logistic regressor and support vector classifier model is created. 
5-fold cross validation (CV) is performed on each model and scores are returned.

As discussed above, F1 score is to be the primary metric to evaluate model performance, 
with precision as a secondary metric. this is reflected in the scoring metrics used in CV

Finally, a Confusion matrix is generated for each model to give an idea of false positive vs false negative rate. 

![]() -image of confusion matrix


### 4.2.4. Balanced model testing

Step 4.2.3 is repeated, this time creating balanced logistic regressor and SVC  models,
with all other hyperparameters held the same. Confusion matrices are once again generated.
The goal of this step is to gain an understanding of how much accuracy is sacrificed at the benefit of improving F1 score.

![]() - confusion matrix image

### 4.2.5. Model Evaluation

With baseline models created, they are evaluated according to the criteria set above.
CV scores of each model are presented below. 
First, standard deviation is shown verify there are no abnormally performing models. 
Scores are then presented in the table below

[table of model scores]

### Model evaluation and selection
Comparing the metrics across models, 
Balanced logistic regression yields the highest recall
and with second to the highest f1_score. 
For this report, we choose to proceed with `LogisticRegression(class_weight="balanced")` 
and optimize the f1 score metric along with optimizing recall
as we want to minimize False Negatives, which is more damaging in medical diagnosis than False Positives.


### 4.2.6. Hyperparameter optimization

Proceeding with a balanced logistic regressor model, 
we now perform hyperparameter optimization.
Logistic regressors has only one hyperparameter C, 
which is sampled from a loguniform distribution. 
optimization is based on F1 score


### Hyperparameter optimization results
[] hyper param table


The hyperparameter optimization made a difference
**('mean_test_score' (0.58) [inline code] compared to 'mean_test_score of cross' (0.54) [inline code] of previous mean validation score)**.
However, the cross-validation scores among the top three models are approximately equivalent. 
This suggests that the model's performance is relatively stable across the parameter space, 
indicating that further tuning may not yield substantial improvements.


### 4.2.7 Final model Scoring and Evaluation

With hyperparameters selected the best model is fitted on the training set,
then scored on both data sets. 
F1 score, Recall score and Accuracy are all computed across both data sets.



## 5. Results/Discussion:

The model created is promising. 
Applying it on our data set gave approximately 68 [inline code] percent accuracy. 
This value is close to our baseline dummy accuracy, 
however with hyper parameter tuning the model achieved a higher F1 score 
compared to original model (0.68 [inline code] on train F1 score compared to 0.61 [inline code] F1 score for base model cv score). 
The final F1 score on test data is found to be 0.533 [inlinecode].
Most importantly for our application, 
the final model performed moderately well at minimizing false negatives. 
The Recall score for our final model applied to the training data set was 0.830, 
(improving from the 0.73 [inline code] recall of the original model). 
On our testing data set, the model performed moderately well, 
returning a recall value of 0.500 [inline code].

The discrepancy between training and testing score 
may be due to the fact that the test data set was quite small (22 examples). 
To get more rigorous performance testing and confidence in our result, 
it would be recommended to seek further data.




## 6. Conclusion

The model created showed some promise, 
being able to correctly classify presence of angiographic coronary disease with a decent level of accuracy (~68%). 
As well, the model performed moderately well on F1 score and was able to minimize the number of false negatives classified 
(recall around 50.0%)

There are some limitations to this report that should be noted 
both at the analysis level and application level.

On the analysis side, only 2 models were tested. 
While their performance was encouraging, 
a more rigorous approach would test a variety of classifiers 
before proceeding with logistic regression.

As well, further hyperparameter optimization could be conducted. 
While a wide range of C-values were tested, 
Only 50 possible values were tested from this range. 
An improvement to this would be to randomly sample from a log-uniform distribution 
to obtain our best C value.

Lastly and perhaps most importantly, 
there is a large discrepancy between our test data set relative to the training data. 
The most reasonable explanation for this would be an test set too small to be representative. 

On the application side, 
we should note that this model was tested specifically for one type of heart disease. 
The scope of the data used in training should be taken into account before proceeding with prediction on new data. 
As well this model requires a significant amount of medical information about a patient in order to create a prediction. 
Most of the information used to create the features to train the model is obtained through angiography, 
a process which itself ends in a diagnosis of the disease.
So it is worth noting that even a high performing model will not be immediately applicable, 
though it gives confidence on the process. 


Overall, we recommend further pursuing optimization of this model. 
Due to the high discrepancy between training and testing scores, 
we would strongly recommend performing further testing on the model on new, 
larger data sets before proceeding with it.


## 7. References

