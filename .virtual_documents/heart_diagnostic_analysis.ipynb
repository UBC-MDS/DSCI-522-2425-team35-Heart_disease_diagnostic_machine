


import numpy as np
import pandas as pd 
import altair as alt
import seaborn as sns
import matplotlib.pyplot as plt
import itertools





df = pd.read_csv("data/raw/pretransformed_heart_disease.csv")
df.head()


df.info()


df.columns = df.columns.str.strip().str.lower().str.replace(" ", "_")


numeric_columns = [
    'age_(in_years)', 
    'resting_blood_pressure_(in_mm_hg_on_admission_to_the_hospital)', 
    'serum_cholesterol_(in_mg/dl)', 
    'maximum_heart_rate_achieved'
]

pair_plots = []
for x_col, y_col in itertools.combinations(numeric_columns, 2):
    chart = alt.Chart(df).mark_circle(size=60).encode(
        alt.X(f'{x_col}:Q', title=x_col.replace("_", " ").capitalize()),
        alt.Y(f'{y_col}:Q', title=y_col.replace("_", " ").capitalize()),
        color='diagnosis_of_heart_disease:N',
        tooltip=[x_col, y_col, 'diagnosis_of_heart_disease']
    ).properties(
        width=200,
        height=200
    )
    pair_plots.append(chart)

pair_plot_grid = alt.vconcat(*[
    alt.hconcat(*pair_plots[i:i + len(numeric_columns) - 1])
    for i in range(0, len(pair_plots), len(numeric_columns) - 1)
])

pair_plot_grid



# Age distribution
age_dist = alt.Chart(df).mark_bar().encode(
    alt.X('age_(in_years):Q', bin=True, title="Age (Years)"),
    alt.Y('count()', title="Count"),
    tooltip=['count()']
).properties(
    title="Age Distribution",
    width=400,
    height=300
)

age_dist



# Diagnosis of Heart Disease
diagnosis_sex = alt.Chart(df).mark_bar().encode(
    alt.X('diagnosis_of_heart_disease:N', title="Heart Disease Diagnosis", axis=alt.Axis(labelAngle=0)),
    alt.Y('count()', title="Count"),
    color='sex:N',
    tooltip=['count()', 'sex']
).properties(
    title="Heart Disease Diagnosis by Sex",
    width=400,
    height=300
)

diagnosis_sex


# Resting Blood Pressure Distribution
resting_bp = alt.Chart(df).mark_bar().encode(
    alt.X('resting_blood_pressure_(in_mm_hg_on_admission_to_the_hospital):Q', bin=True, title="Resting Blood Pressure (mm Hg)"),
    alt.Y('count()', title="Count"),
    tooltip=['count()']
).properties(
    title="Resting Blood Pressure Distribution",
    width=400,
    height=300
)

resting_bp


# 4. Cholesterol Levels by Sex
cholesterol_sex = alt.Chart(df).mark_boxplot().encode(
    alt.X('sex:N', title="Sex"),
    alt.Y('serum_cholesterol_(in_mg/dl):Q', title="Cholesterol (mg/dl)"),
    color='sex:N',
    tooltip=['serum_cholesterol_(in_mg/dl)', 'sex']
).properties(
    title="Cholesterol Levels by Sex",
    width=400,
    height=300
)

cholesterol_sex



# 5. Maximum Heart Rate Distribution
heart_rate = alt.Chart(df).mark_bar().encode(
    alt.X('maximum_heart_rate_achieved:Q', bin=True, title="Maximum Heart Rate"),
    alt.Y('count()', title="Count"),
    tooltip=['count()']
).properties(
    title="Maximum Heart Rate Distribution",
    width=400,
    height=300
)

heart_rate



# 6. Diagnosis by Chest Pain Type
chest_pain_diag = alt.Chart(df).mark_bar().encode(
    alt.X('chest_pain_type:N', title="Chest Pain Type", axis=alt.Axis(labelAngle=0)),
    alt.Y('count()', title="Count"),
    color='diagnosis_of_heart_disease:N',
    tooltip=['count()', 'chest_pain_type', 'diagnosis_of_heart_disease']
).properties(
    title="Heart Disease Diagnosis by Chest Pain Type",
    width=400,
    height=300
)

chest_pain_diag




# 7. ST Depression by Diagnosis
st_depression = alt.Chart(df).mark_boxplot().encode(
    alt.X('diagnosis_of_heart_disease:N', title="Heart Disease Diagnosis", axis=alt.Axis(labelAngle=0)),
    alt.Y('st_depression_induced_by_exercise_relative_to_rest:Q', title="ST Depression"),
    color='diagnosis_of_heart_disease:N',
    tooltip=['st_depression_induced_by_exercise_relative_to_rest', 'diagnosis_of_heart_disease']
).properties(
    title="ST Depression by Diagnosis",
    width=400,
    height=300
)

st_depression


# 8. Number of Major Vessels by Chest Pain Type
vessels_chest_pain = alt.Chart(df).mark_boxplot().encode(
    alt.X('chest_pain_type:N', title="Chest Pain Type"),
    alt.Y('number_of_major_vessels_(0–3)_colored_by_fluoroscopy:Q', title="Number of Major Vessels"),
    color='chest_pain_type:N',
    tooltip=['number_of_major_vessels_(0–3)_colored_by_fluoroscopy', 'chest_pain_type']
).properties(
    title="Number of Major Vessels by Chest Pain Type",
    width=400,
    height=300
)

vessels_chest_pain



# 9. Diagnosis Distribution
diagnosis_dist = alt.Chart(df).mark_bar().encode(
    alt.X('diagnosis_of_heart_disease:N', title="Heart Disease Diagnosis", axis=alt.Axis(labelAngle=0)),
    alt.Y('count()', title="Count"),
    color='diagnosis_of_heart_disease:N',
    tooltip=['count()']
).properties(
    title="Heart Disease Diagnosis Distribution",
    width=400,
    height=300
)

diagnosis_dist






# Import modules
import pandas as pd
from sklearn.compose import make_column_transformer
from sklearn.pipeline import make_pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
 
from sklearn.model_selection import RandomizedSearchCV, cross_validate, train_test_split


#Import data
data = pd.read_csv('data/raw/pretransformed_heart_disease.csv')
data.head()


# Transform target column to True/False with new label of 
# '> 50% diameter narrowing'

data['Diagnosis of heart disease'] = data['Diagnosis of heart disease'].replace({'< 50% diameter narrowing': False, '> 50% diameter narrowing': True})
data.rename(columns={'Diagnosis of heart disease': '> 50% diameter narrowing'}, inplace=True)
data.head()


train_df, test_df = train_test_split(data, test_size=0.1)

X_train = train_df.drop('> 50% diameter narrowing', axis=1)
y_train = train_df['> 50% diameter narrowing']
X_test = test_df.drop('> 50% diameter narrowing', axis=1)
y_test = test_df['> 50% diameter narrowing']


# Lists of feature names
categorical_features = ['Sex', 
                        'Chest pain type', 
                        'Fasting blood sugar > 120 mg/dl', 
                        'Resting electrocardiographic results', 
                        'Exercise-induced angina', 
                        'Slope of the peak exercise ST segment', 
                        'Thalassemia']
numeric_features = list(set(X_train.columns) - set(categorical_features))

# Create transformer pipeline
categorical_transformer = make_pipeline(
    SimpleImputer(strategy="most_frequent", fill_value="missing"),
    OneHotEncoder(handle_unknown="ignore", drop='if_binary', dtype=int, sparse_output=False),
)

numeric_transformer = make_pipeline(
    SimpleImputer(strategy="median", fill_value="missing"),
    StandardScaler(),
)

# Create the column transformer
preprocessor = make_column_transformer(
    (categorical_transformer, categorical_features),
    (numeric_transformer, numeric_features),
)
# Show the preprocessor
preprocessor


# 0. Dummy model
# Imports
from sklearn.dummy import DummyClassifier
from sklearn.metrics import (make_scorer, 
                            precision_score,
                            recall_score,
                            f1_score)

classification_metrics = {
    "accuracy": "accuracy",
    "precision": make_scorer(precision_score, pos_label=True),
    "recall": make_scorer(recall_score, pos_label=True),
    "f1": make_scorer(f1_score, pos_label=True),
}

# The dummy model
dc = make_pipeline(preprocessor, DummyClassifier())
# The mean and std of the cross validated scores for all metrics as a dataframe
cross_val_results = {}
cross_val_results['dummy'] = pd.DataFrame(cross_validate(dc, 
                                                         X_train, 
                                                         y_train, 
                                                         cv=5, 
                                                         scoring=classification_metrics,
                                                         )).agg(['mean', 'std']).round(3).T

# Show the train and validation scores
cross_val_results['dummy']


# 1. Logistic Regression model

# The logreg model pipeline
logreg = make_pipeline(preprocessor, LogisticRegression(random_state=123, max_iter=1000))

# The mean and std of the cross validated scores for all metrics as a dataframe
cross_val_results['logreg'] = pd.DataFrame(cross_validate(logreg, 
                                                          X_train, 
                                                          y_train, 
                                                          cv=5, 
                                                          scoring=classification_metrics, 
                                                          return_train_score=True)).agg(['mean', 'std']).round(3).T

# Show the train and validation scores
cross_val_results['logreg'] 


# 2. Support vector classifier
# Imports
from sklearn.svm import SVC

# The svc model pipeline
svc = make_pipeline(preprocessor, SVC(random_state=123))

# The mean and std of the cross validated scores for all metrics as a dataframe
cross_val_results['svc'] = pd.DataFrame(cross_validate(logreg, 
                                                      X_train, 
                                                      y_train, 
                                                      cv=5, 
                                                      scoring=classification_metrics, 
                                                      return_train_score=True)).agg(['mean', 'std']).round(3).T

# Show the train and validation scores
cross_val_results['svc'] 


# 3. Confusion matrix for the logistic regression
# Imports
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.model_selection import cross_val_predict

confmat_logreg = ConfusionMatrixDisplay.from_predictions(
    y_train,
    cross_val_predict(logreg, X_train, y_train),
    values_format='d'
)
# Show the matrix
confmat_logreg


# 3. Confusion matrix for the SVC
confmat_svc = ConfusionMatrixDisplay.from_predictions(
    y_train,
    cross_val_predict(svc, X_train, y_train),
    values_format='d',
)
# Show the matrix
confmat_svc


# 4. Balanced logistic regression
# The logreg model pipeline
logreg_bal = make_pipeline(preprocessor, LogisticRegression(random_state=123, max_iter=1000, class_weight="balanced"))

# The mean and std of the cross validated scores for all metrics as a dataframe
cross_val_results['logreg_bal'] = pd.DataFrame(cross_validate(logreg_bal, 
                                                        X_train, 
                                                        y_train, 
                                                        cv=5, 
                                                        scoring=classification_metrics, 
                                                        return_train_score=True)).agg(['mean', 'std']).round(3).T

# Show the train and validation scores
cross_val_results['logreg_bal'] 


# 5. Balanced support vector classifier
# The svc model pipeline
svc_bal = make_pipeline(preprocessor, SVC(random_state=123, class_weight="balanced"))

# The mean and std of the cross validated scores for all metrics as a dataframe
cross_val_results['svc_bal'] = pd.DataFrame(cross_validate(svc_bal, 
                                                          X_train, 
                                                          y_train, 
                                                          cv=5, 
                                                          scoring=classification_metrics, 
                                                          return_train_score=True)).agg(['mean', 'std']).round(3).T

# Show the train and validation scores
cross_val_results['svc_bal'] 


# 6. Confusion matrix for the balanced logistic regression

confmat_logreg_bal = ConfusionMatrixDisplay.from_predictions(
    y_train,
    cross_val_predict(logreg_bal, X_train, y_train),
    values_format='d'
)
# Show the matrix
confmat_logreg_bal


# 6. Confusion matrix for the balanced SVC
confmat_svc_bal = ConfusionMatrixDisplay.from_predictions(
    y_train,
    cross_val_predict(svc_bal, X_train, y_train),
    values_format='d'
)
# Show the matrix
confmat_svc_bal


# Manual check that the cross val std doesn't look way off for some model
pd.concat(
    cross_val_results,
    axis='columns'  # Get the right model names and mean/std as columns
).xs(
    'std',  # Select only the 'std' columns
    axis='columns',  # Cross-section the columns
    level=1  # The 1st level ('mean', 'std') instead of the 0th level (the model names)
).style.format(
    precision=2  # Pandas `.style` does not honor previous rounding via `.round()`
).background_gradient(
    axis=None  # Color cells based on the entire matrix rather than row/column-wise
)


# Compare the average scores of all the models
pd.concat(
    cross_val_results,
    axis='columns'
).xs(
    'mean',
    axis='columns',
    level=1
).style.format(
    precision=2
).background_gradient(
    axis=None
)





# Hyperparameter optimization
import numpy as np

param_distributions = {
    'svc__C': np.logspace(-5, 5, 20),  # Range of C values
    'svc__kernel': ['linear', 'rbf', 'poly'],  # Kernel options
    'svc__gamma': ['scale', 'auto'] + list(np.logspace(-3, 2, 10)),  # Gamma values
    'svc__degree': [2, 3, 4],  # Only relevant for 'poly' kernel
}

random_search = RandomizedSearchCV(svc, param_distributions=param_distributions,  
                                    n_iter=100, n_jobs= -1,
                                    scoring='precision',
                                    return_train_score=True,
                                    random_state=522)

random_search.fit(X_train, y_train)


# Extract cv_results_ into a DataFrame
results_df = pd.DataFrame(random_search.cv_results_)
top_results = results_df.sort_values(by='rank_test_score').head(3)
top_results = top_results[[
    'mean_train_score', 'std_train_score', 'mean_test_score', 'std_test_score', 'mean_fit_time', 'params', 
]]

# Set pandas options to display all column content
pd.set_option('display.max_colwidth', None)
top_results





# Fit and predict the test dataset with our best model
best_model = random_search.best_estimator_
best_model.fit(X_train, y_train)


# Precision Metrics
from sklearn.metrics import precision_score

# Predict labels for train and test sets
train_predictions = best_model.predict(X_train)
test_predictions = best_model.predict(X_test)

# Compute precision explicitly
train_precision = precision_score(y_train, train_predictions, average='binary')
test_precision = precision_score(y_test, test_predictions, average='binary')

print("Random Search best model precision: %0.3f" % random_search.best_score_)
print("Train precision on the full train set: %0.3f" % train_precision)
print("Test precision on the full test set: %0.3f" % test_precision)


# Accuracy Metrics
random_search_best_score = random_search.best_score_
train_score = best_model.score(X_train, y_train)
test_score = best_model.score(X_test, y_test)
...
print("Random Search best model score: %0.3f" % random_search_best_score)
print("Train score on the full train set: %0.3f" % train_score)
print("Test score on the full test set: %0.3f" % test_score)





#Title



#summary


#Methods / results


#Discussion


#References
