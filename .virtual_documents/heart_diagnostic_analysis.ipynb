











import numpy as np
import pandas as pd 
import altair as alt
import seaborn as sns
import matplotlib.pyplot as plt
import itertools
import altair_ally as aly
import pandera as pa
from scipy import stats
from sklearn.compose import make_column_transformer
from sklearn.pipeline import make_pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import RandomizedSearchCV, cross_validate, train_test_split
from deepchecks.tabular.checks import FeatureDrift








df = pd.read_csv("data/raw/pretransformed_heart_disease.csv")
df.head()





df = df[df['Diagnosis of heart disease'] <= 1]
df['Diagnosis of heart disease'] = df['Diagnosis of heart disease'].replace(
    {0: '< 50% diameter narrowing', 1: '> 50% diameter narrowing'})


#Validation data step: 
# - check correct column types, 
# - check missingess threshold for most columns, 
# - check no outlier values (max heart rate achieved)
# - check correct expected values for categorical columns
# - remove unexpected values

schema = pa.DataFrameSchema(
    {
        "Age (in years)": pa.Column(int,
                                    pa.Check(lambda s: s.isna().mean() <= 0.05, 
                                        element_wise=False,
                                        error="Too many null values in column."), 
                                    nullable=True),
        "Sex": pa.Column(str,pa.Check.isin(["male", "female"])),
        "Chest pain type": pa.Column(str, pa.Check.isin(["typical angina", "atypical angina", "non-anginal pain", "asymptomatic"])),
        "Resting blood pressure (in mm Hg on admission to the hospital)": pa.Column(int,
                                                                            pa.Check(lambda s: s.isna().mean() <= 0.05, 
                                                                                element_wise=False, 
                                                                                error="Too many null values in column."), 
                                                                            nullable=True),
        "Serum cholesterol (in mg/dl)": pa.Column(int,
                                            pa.Check(lambda s: s.isna().mean() <= 0.05, 
                                                element_wise=False, 
                                                error="Too many null values in column."), 
                                            nullable=True),
        "Fasting blood sugar > 120 mg/dl": pa.Column(bool,
                                                pa.Check(lambda s: s.isna().mean() <= 0.05, 
                                                    element_wise=False, 
                                                    error="Too many null values in column."), 
                                                nullable=True),
        "Resting electrocardiographic results": pa.Column(str,pa.Check.isin(["normal", 
                                                                             "having ST-T wave abnormality", 
                                                                             "showing probable or definite left ventricular hypertrophy by Estes' criteria"])),
        "Maximum heart rate achieved": pa.Column(int,
                                            pa.Check.between(75, 250), 
                                            nullable=True),
        "Exercise-induced angina": pa.Column(str, pa.Check.isin(["yes","no"])),
        "ST depression induced by exercise relative to rest": pa.Column(float,
                                                                pa.Check(lambda s: s.isna().mean() <= 0.05, 
                                                                    element_wise=False, 
                                                                    error="Too many null values in column."), 
                                                                nullable=True),
        "Slope of the peak exercise ST segment": pa.Column(str, pa.Check.isin(["upsloping", "flat", "downsloping"])),
        "Number of major vessels (0–3) colored by fluoroscopy": pa.Column(float,
                                                                    pa.Check(lambda s: s.isna().mean() <= 0.05, 
                                                                    element_wise=False, 
                                                                    error="Too many null values in column."),
                                                                nullable=True),
        "Thalassemia": pa.Column(str, 
                                 pa.Check(lambda s: s.isna().mean() <= 0.05, 
                                        element_wise=False, 
                                        error="Too many null values in column."),
                                    nullable=True),
        "Diagnosis of heart disease": pa.Column(str, pa.Check.isin(["< 50% diameter narrowing", "> 50% diameter narrowing"]))
    } ,
    checks=[
        pa.Check(lambda df: ~df.duplicated().any(), error="Duplicate rows found."),
        pa.Check(lambda df: ~(df.isna().all(axis=1)).any(), error="Empty rows found.")
    ],
    drop_invalid_rows=True
)

df = schema.validate(df, lazy = True)
df



train_df, test_df = train_test_split(df, test_size=0.1)

X_train = train_df.drop('Diagnosis of heart disease', axis=1)
y_train = train_df['Diagnosis of heart disease']
X_test = test_df.drop('Diagnosis of heart disease', axis=1)
y_test = test_df['Diagnosis of heart disease']


#verify correlations - Feature-target:
from deepchecks.tabular import Dataset
from deepchecks.tabular.checks import FeatureLabelCorrelation

train_ds = Dataset(train_df, label="Diagnosis of heart disease", cat_features=['Sex','Chest pain type',
                                                                               'Fasting blood sugar > 120 mg/dl',
                                                                               'Resting electrocardiographic results',
                                                                               'Exercise-induced angina',
                                                                               'Slope of the peak exercise ST segment',
                                                                               'Thalassemia'])
test_ds = Dataset(test_df, label="Diagnosis of heart disease", cat_features=['Sex','Chest pain type',
                                                                           'Fasting blood sugar > 120 mg/dl',
                                                                           'Resting electrocardiographic results',
                                                                           'Exercise-induced angina',
                                                                           'Slope of the peak exercise ST segment',
                                                                           'Thalassemia'])


check_feat_lab_corr = FeatureLabelCorrelation().add_condition_feature_pps_less_than(0.9)
check_feat_lab_corr_result = check_feat_lab_corr.run(dataset=train_ds)
check_feat_lab_corr_result.show(show_additional_outputs=False)


#verify correlations: feature-feature:
# reference: https://docs.deepchecks.com/stable/tabular/auto_checks/data_integrity/plot_feature_feature_correlation.html#sphx-glr-tabular-auto-checks-data-integrity-plot-feature-feature-correlation-py

from deepchecks.tabular.datasets.classification import adult
from deepchecks.tabular.checks.data_integrity import FeatureFeatureCorrelation

check = FeatureFeatureCorrelation()
# check.run(df)

check.add_condition_max_number_of_pairs_above_threshold(0.8, 3)
result = check.run(train_ds)
result.show(show_additional_outputs=False)





check = FeatureDrift()
result = check.run(train_dataset=train_ds, test_dataset=test_ds)
result.value











train_df.info()


train_df.columns = train_df.columns.str.strip().str.lower().str.replace(" ", "_")
test_df.columns = test_df.columns.str.strip().str.lower().str.replace(" ", "_")


alt.data_transformers.enable('default', max_rows=None)
alt.renderers.enable('html')


# Univariate distrbution for the quantitative variables
numeric_columns = [
    'age_(in_years)',
    'resting_blood_pressure_(in_mm_hg_on_admission_to_the_hospital)',
    'serum_cholesterol_(in_mg/dl)',
    'maximum_heart_rate_achieved',
    'st_depression_induced_by_exercise_relative_to_rest',
    'number_of_major_vessels_(0–3)_colored_by_fluoroscopy'
]

aly.alt.data_transformers.enable('vegafusion')


aly.dist(train_df[numeric_columns + ['diagnosis_of_heart_disease']], color='diagnosis_of_heart_disease')





categorical_columns = [
    'sex', 
    'chest_pain_type', 
    'fasting_blood_sugar_>_120_mg/dl', 
    'resting_electrocardiographic_results',
    'exercise-induced_angina', 
    'slope_of_the_peak_exercise_st_segment', 
    'thalassemia'
]

# Visualize the distributions for the categorical variables colored by 'diagnosis_of_heart_disease'
aly.dist(
    train_df[categorical_columns + ['diagnosis_of_heart_disease']]
    .assign(diagnosis_of_heart_disease=lambda x: x['diagnosis_of_heart_disease'].astype(object)), 
    dtype='object', 
    color='diagnosis_of_heart_disease'
)





# Visualize pairwise correlations for the quantitative variables
aly.corr(train_df[numeric_columns])





# Select numeric columns with high correlations
columns_with_at_least_one_high_corr = [
    'age_(in_years)',
    'resting_blood_pressure_(in_mm_hg_on_admission_to_the_hospital)',
    'serum_cholesterol_(in_mg/dl)',
    'maximum_heart_rate_achieved',
    'st_depression_induced_by_exercise_relative_to_rest',
    'number_of_major_vessels_(0–3)_colored_by_fluoroscopy',
    'diagnosis_of_heart_disease'
]

sample_size = min(len(train_df), 300)

aly.pair(
    train_df[columns_with_at_least_one_high_corr].sample(sample_size),
    color='diagnosis_of_heart_disease'
)











# Lists of feature names
categorical_features = ['Sex', 
                        'Chest pain type', 
                        'Fasting blood sugar > 120 mg/dl', 
                        'Resting electrocardiographic results', 
                        'Exercise-induced angina', 
                        'Slope of the peak exercise ST segment', 
                        'Thalassemia']
numeric_features = list(set(X_train.columns) - set(categorical_features))

# Create transformer pipeline
categorical_transformer = make_pipeline(
    SimpleImputer(strategy="most_frequent", fill_value="missing"),
    OneHotEncoder(handle_unknown="ignore", drop='if_binary', dtype=int, sparse_output=False),
)

numeric_transformer = make_pipeline(
    SimpleImputer(strategy="median", fill_value="missing"),
    StandardScaler(),
)

# Create the column transformer
preprocessor = make_column_transformer(
    (categorical_transformer, categorical_features),
    (numeric_transformer, numeric_features),
)
# Show the preprocessor
preprocessor





# 0. Dummy model
# Imports
from sklearn.dummy import DummyClassifier
from sklearn.metrics import (make_scorer, 
                            precision_score,
                            recall_score,
                            f1_score)

classification_metrics = {
    "accuracy": "accuracy",
    "precision": make_scorer(precision_score, pos_label='> 50% diameter narrowing'),
    "recall": make_scorer(recall_score, pos_label='> 50% diameter narrowing'),
    "f1": make_scorer(f1_score, pos_label='> 50% diameter narrowing'),
}

# The dummy model
dc = make_pipeline(DummyClassifier())
# The mean and std of the cross validated scores for all metrics as a dataframe
cross_val_results = {}
cross_val_results['dummy'] = pd.DataFrame(cross_validate(dc, 
                                                         X_train, 
                                                         y_train, 
                                                         cv=5, 
                                                         scoring=classification_metrics,
                                                         )).agg(['mean', 'std']).round(3).T

# Show the train and validation scores
cross_val_results['dummy']


# 1. Logistic Regression model

# The logreg model pipeline
logreg = make_pipeline(preprocessor, LogisticRegression(random_state=123, max_iter=1000))

# The mean and std of the cross validated scores for all metrics as a dataframe
cross_val_results['logreg'] = pd.DataFrame(cross_validate(logreg, 
                                                          X_train, 
                                                          y_train, 
                                                          cv=5, 
                                                          scoring=classification_metrics, 
                                                          return_train_score=True)).agg(['mean', 'std']).round(3).T

# Show the train and validation scores
cross_val_results['logreg'] 


# 2. Support vector classifier
# Imports
from sklearn.svm import SVC

# The svc model pipeline
svc = make_pipeline(preprocessor, SVC(random_state=123))

# The mean and std of the cross validated scores for all metrics as a dataframe
cross_val_results['svc'] = pd.DataFrame(cross_validate(svc, 
                                                      X_train, 
                                                      y_train, 
                                                      cv=5, 
                                                      scoring=classification_metrics, 
                                                      return_train_score=True)).agg(['mean', 'std']).round(3).T

# Show the train and validation scores
cross_val_results['svc'] 


# 3. Confusion matrix for the logistic regression
# Imports
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.model_selection import cross_val_predict

confmat_logreg = ConfusionMatrixDisplay.from_predictions(
    y_train,
    cross_val_predict(logreg, X_train, y_train),
    values_format='d'
)
# Show the matrix
confmat_logreg


# 3. Confusion matrix for the SVC
confmat_svc = ConfusionMatrixDisplay.from_predictions(
    y_train,
    cross_val_predict(svc, X_train, y_train),
    values_format='d',
)
# Show the matrix
confmat_svc





# 4. Balanced logistic regression
# The logreg model pipeline
logreg_bal = make_pipeline(preprocessor, LogisticRegression(random_state=123, max_iter=1000, class_weight="balanced"))

# The mean and std of the cross validated scores for all metrics as a dataframe
cross_val_results['logreg_bal'] = pd.DataFrame(cross_validate(logreg_bal, 
                                                        X_train, 
                                                        y_train, 
                                                        cv=5, 
                                                        scoring=classification_metrics,
                                                        return_train_score=True)).agg(['mean', 'std']).round(3).T

# Show the train and validation scores
cross_val_results['logreg_bal'] 


# 5. Balanced support vector classifier
# The svc model pipeline
svc_bal = make_pipeline(preprocessor, SVC(random_state=123, class_weight="balanced"))

# The mean and std of the cross validated scores for all metrics as a dataframe
cross_val_results['svc_bal'] = pd.DataFrame(cross_validate(svc_bal, 
                                                          X_train, 
                                                          y_train, 
                                                          cv=5, 
                                                          scoring=classification_metrics, 
                                                          return_train_score=True)).agg(['mean', 'std']).round(3).T

# Show the train and validation scores
cross_val_results['svc_bal'] 


# 6. Confusion matrix for the balanced logistic regression

confmat_logreg_bal = ConfusionMatrixDisplay.from_predictions(
    y_train,
    cross_val_predict(logreg_bal, X_train, y_train),
    values_format='d'
)
# Show the matrix
confmat_logreg_bal


# 6. Confusion matrix for the balanced SVC
confmat_svc_bal = ConfusionMatrixDisplay.from_predictions(
    y_train,
    cross_val_predict(svc_bal, X_train, y_train),
    values_format='d'
)
# Show the matrix
confmat_svc_bal





# Manual check that the cross val std doesn't look way off for some model
pd.concat(
    cross_val_results,
    axis='columns'  # Get the right model names and mean/std as columns
).xs(
    'std',  # Select only the 'std' columns
    axis='columns',  # Cross-section the columns
    level=1  # The 1st level ('mean', 'std') instead of the 0th level (the model names)
).style.format(
    precision=2  # Pandas `.style` does not honor previous rounding via `.round()`
).background_gradient(
    axis=None  # Color cells based on the entire matrix rather than row/column-wise
)


# Compare the average scores of all the models
pd.concat(
    cross_val_results,
    axis='columns'
).xs(
    'mean',
    axis='columns',
    level=1
).style.format(
    precision=2
).background_gradient(
    axis=None
)








# Hyperparameter optimization f1
import numpy as np

# Define parameter distributions
param_distributions = {
    'logisticregression__C': np.logspace(-5, 5, 50),  # Regularization strength
}

custom_scorer = make_scorer(f1_score, pos_label='> 50% diameter narrowing')

random_search = RandomizedSearchCV(logreg_bal, param_distributions=param_distributions,
                                    n_iter=100, n_jobs= -1,
                                    scoring=custom_scorer,
                                    return_train_score=True,
                                    random_state=123)

random_search.fit(X_train, y_train)


# Extract cv_results_ into a DataFrame
results_df = pd.DataFrame(random_search.cv_results_)
top_results = results_df.sort_values(by='rank_test_score').head(3)
top_results = top_results[[
    'mean_train_score', 'std_train_score', 'mean_test_score', 'std_test_score', 'mean_fit_time', 'params', 
]]

# Set pandas options to display all column content
pd.set_option('display.max_colwidth', None)
top_results








# Fit and predict the test dataset with our best model
best_model = random_search.best_estimator_
best_model.fit(X_train, y_train)


# f1_score Metrics
from sklearn.metrics import f1_score

# Predict labels for train and test sets
train_predictions = best_model.predict(X_train)
test_predictions = best_model.predict(X_test)

# Compute precision explicitly
train_f1 = f1_score(y_train, train_predictions, average='binary', 
                    pos_label='> 50% diameter narrowing')
test_f1 = f1_score(y_test, test_predictions, average='binary', 
                   pos_label='> 50% diameter narrowing')

print("Random Search best model f1_score: %0.3f" % random_search.best_score_)
print("Train f1_score on the full train set: %0.3f" % train_f1)
print("Test f1_score on the full test set: %0.3f" % test_f1)


# recall Metrics
from sklearn.metrics import recall_score

# Compute precision explicitly
train_recall = recall_score(y_train, train_predictions, average='binary',
                            pos_label='> 50% diameter narrowing')
test_recall = recall_score(y_test, test_predictions, average='binary',
                          pos_label='> 50% diameter narrowing')

print("Train recall on the full train set: %0.3f" % train_recall)
print("Test recall on the full test set: %0.3f" % test_recall)


# Accuracy Metrics
random_search_best_score = random_search.best_score_
train_score = best_model.score(X_train, y_train)
test_score = best_model.score(X_test, y_test)
...

print("Train score on the full train set: %0.3f" % train_score)
print("Test score on the full test set: %0.3f" % test_score)









